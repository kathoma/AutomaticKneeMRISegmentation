{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "sys.path.insert(0, 'lib')\n",
    "import numpy as np\n",
    "import random\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import pydicom\n",
    "from utils import make_giant_mat, make_dictionary, make_echo_dict\n",
    "from shutil import copyfile\n",
    "import nibabel as nib\n",
    "\n",
    "from inference_utils import run_inference\n",
    "from make_inference_csv import *\n",
    "from compare_segmentations import get_dice_scores, get_jaccard_indices, compare_region_means,compare_region_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify which subjects you want to analyze using their OAI Patient ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9543086' '9123289' '9260036' '9435250' '9909311' '9518827' '9013634'\n",
      " '9245760' '9458093' '9405107' '9120358' '9279874' '9376146' '9529761']\n",
      "\n",
      "['9435250' '9013634' '9909311' '9279874' '9260036']\n",
      "[4 8 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/dominik_data/train_val_test/pid_train.pickle', 'rb') as f:    \n",
    "    pid_train = pickle.load(f)\n",
    "    \n",
    "with open('/data/dominik_data/train_val_test/pid_val.pickle', 'rb') as f:    \n",
    "    pid_val = pickle.load(f)\n",
    "      \n",
    "with open('/data/dominik_data/train_val_test/pid_test.pickle', 'rb') as f:    \n",
    "    pid_test = pickle.load(f)\n",
    "    \n",
    "with open('/data/dominik_data/train_val_test/pid_test_expert2.pickle', 'rb') as f:\n",
    "    pid_expert2 = pickle.load(f)\n",
    "    \n",
    "with open('/data/dominik_data/train_val_test/pid_test_expert2_years.pickle', 'rb') as f:\n",
    "    pid_expert2_yrs = pickle.load(f)\n",
    "\n",
    "print(pid_test)\n",
    "print()\n",
    "print(pid_expert2)\n",
    "print(pid_expert2_yrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Pandas Data Frame and CSV file to specify which image files you want to have analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/kevin_data/expert2\n",
      "/data/kevin_data/predicted\n",
      "/data/kevin_data/qmetric\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_dir</th>\n",
       "      <th>seg_path</th>\n",
       "      <th>refined_seg_path</th>\n",
       "      <th>t2_img_path</th>\n",
       "      <th>t2_region_json_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/kevin_data/images/YR4/9435250/T2</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations/94352...</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations_refin...</td>\n",
       "      <td>/data/kevin_data/predicted/t2maps/9435250_4</td>\n",
       "      <td>/data/kevin_data/predicted/region_means/943525...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/kevin_data/images/YR8/9013634/T2</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations/90136...</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations_refin...</td>\n",
       "      <td>/data/kevin_data/predicted/t2maps/9013634_8</td>\n",
       "      <td>/data/kevin_data/predicted/region_means/901363...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/kevin_data/images/YR8/9909311/T2</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations/99093...</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations_refin...</td>\n",
       "      <td>/data/kevin_data/predicted/t2maps/9909311_8</td>\n",
       "      <td>/data/kevin_data/predicted/region_means/990931...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/kevin_data/images/YR4/9279874/T2</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations/92798...</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations_refin...</td>\n",
       "      <td>/data/kevin_data/predicted/t2maps/9279874_4</td>\n",
       "      <td>/data/kevin_data/predicted/region_means/927987...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/kevin_data/images/YR8/9260036/T2</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations/92600...</td>\n",
       "      <td>/data/kevin_data/predicted/segmentations_refin...</td>\n",
       "      <td>/data/kevin_data/predicted/t2maps/9260036_8</td>\n",
       "      <td>/data/kevin_data/predicted/region_means/926003...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  img_dir  \\\n",
       "0  /data/kevin_data/images/YR4/9435250/T2   \n",
       "0  /data/kevin_data/images/YR8/9013634/T2   \n",
       "0  /data/kevin_data/images/YR8/9909311/T2   \n",
       "0  /data/kevin_data/images/YR4/9279874/T2   \n",
       "0  /data/kevin_data/images/YR8/9260036/T2   \n",
       "\n",
       "                                            seg_path  \\\n",
       "0  /data/kevin_data/predicted/segmentations/94352...   \n",
       "0  /data/kevin_data/predicted/segmentations/90136...   \n",
       "0  /data/kevin_data/predicted/segmentations/99093...   \n",
       "0  /data/kevin_data/predicted/segmentations/92798...   \n",
       "0  /data/kevin_data/predicted/segmentations/92600...   \n",
       "\n",
       "                                    refined_seg_path  \\\n",
       "0  /data/kevin_data/predicted/segmentations_refin...   \n",
       "0  /data/kevin_data/predicted/segmentations_refin...   \n",
       "0  /data/kevin_data/predicted/segmentations_refin...   \n",
       "0  /data/kevin_data/predicted/segmentations_refin...   \n",
       "0  /data/kevin_data/predicted/segmentations_refin...   \n",
       "\n",
       "                                   t2_img_path  \\\n",
       "0  /data/kevin_data/predicted/t2maps/9435250_4   \n",
       "0  /data/kevin_data/predicted/t2maps/9013634_8   \n",
       "0  /data/kevin_data/predicted/t2maps/9909311_8   \n",
       "0  /data/kevin_data/predicted/t2maps/9279874_4   \n",
       "0  /data/kevin_data/predicted/t2maps/9260036_8   \n",
       "\n",
       "                                 t2_region_json_path  \n",
       "0  /data/kevin_data/predicted/region_means/943525...  \n",
       "0  /data/kevin_data/predicted/region_means/901363...  \n",
       "0  /data/kevin_data/predicted/region_means/990931...  \n",
       "0  /data/kevin_data/predicted/region_means/927987...  \n",
       "0  /data/kevin_data/predicted/region_means/926003...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pd = make_expert_csv_all_years(pID=pid_test,img_dir='/data/kevin_data/images', dir_to_save='/data/kevin_data/predicted')\n",
    "\n",
    "expert1_pd = make_expert_csv_all_years(pID=pid_test, img_dir='/data/kevin_data/images',dir_to_save='/data/kevin_data/qmetric')\n",
    "\n",
    "# We only have segmentations for one timepoint for the subjects segmented by expert2 and these are nifti files\n",
    "expert2_pd = make_expert_csv_specific_years(pID=pid_expert2, \n",
    "                                            years=pid_expert2_yrs, \n",
    "                                            img_dir='/data/kevin_data/images', \n",
    "                                            dir_to_save='/data/kevin_data/expert2', \n",
    "                                            seg_provided=True, \n",
    "                                            seg_format = \"numpy\")\n",
    "\n",
    "predict_subset_pd = make_expert_csv_specific_years(pID=pid_expert2, \n",
    "                                            years=pid_expert2_yrs, \n",
    "                                            img_dir='/data/kevin_data/images', \n",
    "                                            dir_to_save='/data/kevin_data/predicted', \n",
    "                                            seg_provided=True, \n",
    "                                            seg_format = \"numpy\",\n",
    "                                            csv_filename = 'file_paths_subset.csv')\n",
    "\n",
    "expert1_subset_pd = make_expert_csv_specific_years(pID=pid_expert2, \n",
    "                                            years=pid_expert2_yrs, \n",
    "                                            img_dir='/data/kevin_data/images', \n",
    "                                            dir_to_save='/data/kevin_data/qmetric', \n",
    "                                            seg_provided=True, \n",
    "                                            seg_format = \"numpy\",\n",
    "                                            csv_filename = 'file_paths_subset.csv')\n",
    "predict_subset_pd.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate T2 maps for your MESE images\n",
    "### This is the fun part!\n",
    "- If you provide a value for the 'expert_pd' argument, it will use your provided segmentations\n",
    "\n",
    "- If you provide a value for the 'to_segment_pd' argument, it will automatically segment the cartilage and then use that auto-segmentation to generate the T2 maps. By default, this uses our trained model, but model weights can bbe changed via the 'model_weights_file' argument and the model can be changed by altering the inference.get_model function in inference.py. \n",
    "\n",
    "- In addition to generating 3D T2 maps, it also provides the segmentations used to generate those T2 maps as 3D numpy arrays and json files that summarize the avg T2 value in each anatomical region of the cartilage plate\n",
    "\n",
    "- These results are all saved in the destinations specied in your Pandas dataframe (expert_pd or to_segment_pd) that you made in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Automatically segmenting images and using those segmentations to analyze MESE MRIs\n",
      "--------------------------------\n",
      "\n",
      "/data/kevin_data/images/YR4/9543086/T2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Kevin/AutomaticKneeMRISegmentation/models.py:97: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(512, (3, 3), padding=\"same\", strides=(2, 2))`\n",
      "  output_shape=deconv_shape)(conv),\n",
      "/home/Kevin/AutomaticKneeMRISegmentation/models.py:97: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(256, (3, 3), padding=\"same\", strides=(2, 2))`\n",
      "  output_shape=deconv_shape)(conv),\n",
      "/home/Kevin/AutomaticKneeMRISegmentation/models.py:97: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(128, (3, 3), padding=\"same\", strides=(2, 2))`\n",
      "  output_shape=deconv_shape)(conv),\n",
      "/home/Kevin/AutomaticKneeMRISegmentation/models.py:97: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (3, 3), padding=\"same\", strides=(2, 2))`\n",
      "  output_shape=deconv_shape)(conv),\n",
      "/home/Kevin/AutomaticKneeMRISegmentation/models.py:97: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(32, (3, 3), padding=\"same\", strides=(2, 2))`\n",
      "  output_shape=deconv_shape)(conv),\n",
      "/home/shared/anaconda3/envs/python3/lib/python3.7/site-packages/scipy/optimize/minpack.py:808: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/home/shared/anaconda3/envs/python3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/shared/anaconda3/envs/python3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/kevin_data/images/YR8/9543086/T2\n",
      "/data/kevin_data/images/YR4/9123289/T2\n",
      "/data/kevin_data/images/YR8/9123289/T2\n",
      "/data/kevin_data/images/YR4/9260036/T2\n",
      "/data/kevin_data/images/YR8/9260036/T2\n",
      "/data/kevin_data/images/YR4/9435250/T2\n",
      "/data/kevin_data/images/YR8/9435250/T2\n",
      "/data/kevin_data/images/YR4/9909311/T2\n",
      "/data/kevin_data/images/YR8/9909311/T2\n",
      "/data/kevin_data/images/YR4/9518827/T2\n",
      "/data/kevin_data/images/YR8/9518827/T2\n",
      "/data/kevin_data/images/YR4/9013634/T2\n",
      "/data/kevin_data/images/YR8/9013634/T2\n",
      "/data/kevin_data/images/YR4/9245760/T2\n",
      "/data/kevin_data/images/YR8/9245760/T2\n",
      "/data/kevin_data/images/YR4/9458093/T2\n",
      "/data/kevin_data/images/YR8/9458093/T2\n",
      "/data/kevin_data/images/YR4/9405107/T2\n",
      "/data/kevin_data/images/YR8/9405107/T2\n",
      "/data/kevin_data/images/YR4/9120358/T2\n",
      "/data/kevin_data/images/YR8/9120358/T2\n",
      "/data/kevin_data/images/YR4/9279874/T2\n",
      "/data/kevin_data/images/YR8/9279874/T2\n",
      "/data/kevin_data/images/YR4/9376146/T2\n",
      "/data/kevin_data/images/YR8/9376146/T2\n",
      "/data/kevin_data/images/YR4/9529761/T2\n",
      "/data/kevin_data/images/YR8/9529761/T2\n",
      "--------------------------------\n",
      "Using provided segmentations to analyze MESE MRIs\n",
      "--------------------------------\n",
      "\n",
      "/data/kevin_data/qmetric/segmentations/9543086_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9543086_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9123289_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9123289_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9260036_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9260036_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9435250_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9435250_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9909311_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9909311_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9518827_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9518827_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9013634_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9013634_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9245760_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9245760_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9458093_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9458093_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9405107_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9405107_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9120358_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9120358_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9279874_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9279874_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9376146_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9376146_8.npy\n",
      "/data/kevin_data/qmetric/segmentations/9529761_4.npy\n",
      "/data/kevin_data/qmetric/segmentations/9529761_8.npy\n",
      "--------------------------------\n",
      "Using provided segmentations to analyze MESE MRIs\n",
      "--------------------------------\n",
      "\n",
      "/data/kevin_data/expert2/segmentations/9435250_4.npy\n",
      "/data/kevin_data/expert2/segmentations/9013634_8.npy\n",
      "/data/kevin_data/expert2/segmentations/9909311_8.npy\n",
      "/data/kevin_data/expert2/segmentations/9279874_4.npy\n",
      "/data/kevin_data/expert2/segmentations/9260036_8.npy\n"
     ]
    }
   ],
   "source": [
    "run_inference(to_segment_pd = predict_pd)\n",
    "              \n",
    "run_inference(expert_pd = expert1_pd)\n",
    "              \n",
    "run_inference(expert_pd = expert2_pd)\n",
    "\n",
    "# We don't need to generate additional segmentations for the 'predict_subset_pd' or 'expert1_subset_pd' \n",
    "# because they are already generated as part of the 'predict_pd' and 'expert1_pd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip the expert segmentations if you haven't already\n",
    "# for file in os.listdir('/data/kevin_data/expert2/segmentations'):\n",
    "#     if file[-4:]=='.npy':\n",
    "#         temp = np.load(os.path.join('/data/kevin_data/expert2/segmentations',file))\n",
    "#         temp = np.flip(temp, axis = 0)\n",
    "#         np.save(os.path.join('/data/kevin_data/expert2/segmentations',file),temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare two segmentation approaches for a single set of images\n",
    "- Quantify how closely the two segmentations agree with one another using Dice Score and Jaccard Index\n",
    "- Quantify how closely the downstream T2 measurements correlate for each region using Pearson correlation\n",
    "- Quantify the mean absolute difference in T2 measurements for each region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARB0lEQVR4nO3debBkdXnG8e/DDAyyCTjXhWUYXEIES4MZwGiiBqKyKRqtEhRFYjImFY2kXEDRiJZaaLklZSpxIqhxgYpoKsQdUbQ0SDLswogsog6LDEEELBaBN3/0IVwu98707dN3+eH3U9V1T/dZfu873feZ0+ecvp2qQpLUrs0WugBJUj8GuSQ1ziCXpMYZ5JLUOINckhpnkEtS4wxyLXpJ/jnJ2xe6DmmxiteRayEluRp4FHA3cA9wKfCvwJqquncOx90e+BBwMLA1cB1wUlW9b67GlOaKe+RaDJ5fVdsCuwEnAscCJ83xmB8GtgGeCDwceAFw5TgHSLJ0nNuTZmKQa9Goql9V1enAS4GjkjwJIMknk7z7vuWSHJbkgiS3JLkyyYHd4w9PclKS65Jck+TdSZbMMNw+wOeq6pdVdW9V/aiqTps0xl5JzkhyU5JfJHlr9/iyJB9Jcm13+0iSZd28ZydZn+TYJNcDn+geP7Sr9+Yk/5XkyZPGObar9dYklyU5YKz/qPqtYJBr0amq/wbWA380dV6SfRkcenkTsD3wTODqbvanGByieTywN/Bc4M9nGOYHwHuSHJ3kCVPG2Bb4JvA1YKdue2d2s48Hngb8HvAUYF/gbZNWfzSwI4N3F6uTPBU4GXgN8AjgY8Dp3X8IewCvBfbp3pE8b1Iv0tAMci1W1zIIxKleDZxcVWd0e9LXVNWPkjwKOAg4pqp+XVU3MDh8cvgM238d8FkGQXppkiuSHNTNOxS4vqo+WFV3VNWtVXVON+/lwLuq6oaq2gC8E3jFpO3eC7yjqu6sqtuBvwA+VlXnVNU9VfUp4E4G/xncAywD9kyyeVVdXVVjPbyj3w4GuRarnYGbpnl8V6Y/lr0bsDlwXXcI42YGe7+PnG7jVXV7Vb23qn6fwZ7yvwGfT7LjRsaAwR76Tyfd/2n32H02VNUdU+p6w301dXXtCuxUVVcAxwAnADckOTXJ5G1JQzHItegk2YdBkH9vmtk/Bx43w+N3Asuravvutl1V7bWp8arqFuC9DK5e2X0jY8DgncJuk+6v6B77/81NU9d7JtW0fVVtVVWndGN/rqr+sNtmAV41o1kzyLVoJNkuyaHAqcBnquriaRY7CTg6yQFJNkuyc5LfrarrgG8AH+y2s1mSxyV51gxjvT3JPkm2SLIl8HrgZuAy4EvAo5Mc0x3L3jbJft2qpwBvSzKRZDnwd8BnNtLWvwB/mWS/DGyd5JBum3sk2b87WXoHcDuDwy3SrBjkWgz+M8mtDPZej2dwfffR0y3YnQg9msHx718B3+H+PeRXAlswuBb9l8BpwGNmGLMYXFVyI4M96ucAh1TVbVV1a3f/+cD1wOXAH3frvRtYC1wEXAyc1z02/SBVaxkcJ/9oV9MVwKu62csYXG55YzfOI4G3zrQtaSZ+IEiSGuceuSQ1ziCXpMYZ5JLUOINckho3r3/UZ/ny5bVy5cr5HFKSmnfuuefeWFUTM82f1yBfuXIla9eunc8hJal5SX66sfkeWpGkxhnkktQ4g1ySGmeQS1LjDHJJapxBLkmN22SQJzk5yQ1JfjjpsR277zO8vPu5w9yWKUmayTB75J8EDpzy2HHAmVX1BAbfZXjcmOuSJA1pk0FeVd/lwV+5dRiDL7ql+/nCMdclSRrSqJ/sfFT3jSxU1XVJpv1eRIAkq4HVACtWrBhxOEmLycrjvrzQJTTl6hMPmdPtz/nJzqpaU1WrqmrVxMSMfypAkjSiUYP8F0keA9D9vGF8JUmSZmPUID8dOKqbPgr4j/GUI0marWEuPzwFOBvYI8n6JK9m8IWxz0lyOYMvqT1xbsuUJM1kkyc7q+qIGWYdMOZaJEkj8JOdktQ4g1ySGmeQS1LjDHJJapxBLkmNM8glqXEGuSQ1ziCXpMYZ5JLUOINckhpnkEtS4wxySWqcQS5JjTPIJalxBrkkNc4gl6TGGeSS1DiDXJIaZ5BLUuMMcklqnEEuSY0zyCWpcQa5JDXOIJekxhnkktQ4g1ySGmeQS1LjDHJJapxBLkmNM8glqXEGuSQ1ziCXpMb1CvIkf5vkkiQ/THJKki3HVZgkaTgjB3mSnYG/AVZV1ZOAJcDh4ypMkjScvodWlgIPS7IU2Aq4tn9JkqTZWDrqilV1TZIPAD8Dbge+UVXfmLpcktXAaoAVK1aMOpw0p1Ye9+WFLkEaWZ9DKzsAhwG7AzsBWyc5cupyVbWmqlZV1aqJiYnRK5UkTavPoZU/AX5SVRuq6jfAF4Gnj6csSdKw+gT5z4CnJdkqSYADgHXjKUuSNKyRg7yqzgFOA84DLu62tWZMdUmShjTyyU6AqnoH8I4x1SJJGoGf7JSkxhnkktQ4g1ySGmeQS1LjDHJJapxBLkmNM8glqXEGuSQ1ziCXpMYZ5JLUOINckhpnkEtS4wxySWqcQS5JjTPIJalxvf4euRYvv0xY+u3hHrkkNc4gl6TGGeSS1DiDXJIaZ5BLUuMMcklqnEEuSY0zyCWpcQa5JDXOIJekxhnkktQ4g1ySGmeQS1LjDHJJapxBLkmNM8glqXG9gjzJ9klOS/KjJOuS/MG4CpMkDafvNwT9PfC1qnpJki2ArcZQkyRpFkYO8iTbAc8EXgVQVXcBd42nLEnSsPocWnkssAH4RJLzk3w8ydZjqkuSNKQ+Qb4UeCrwT1W1N/Br4LipCyVZnWRtkrUbNmzoMZwkaTp9gnw9sL6qzunun8Yg2B+gqtZU1aqqWjUxMdFjOEnSdEYO8qq6Hvh5kj26hw4ALh1LVZKkofW9auV1wGe7K1auAo7uX5IkaTZ6BXlVXQCsGlMtkqQR+MlOSWqcQS5JjTPIJalxBrkkNc4gl6TGGeSS1DiDXJIaZ5BLUuMMcklqnEEuSY0zyCWpcQa5JDXOIJekxhnkktQ4g1ySGmeQS1LjDHJJapxBLkmNM8glqXEGuSQ1ziCXpMYZ5JLUOINckhpnkEtS4wxySWqcQS5JjTPIJalxBrkkNc4gl6TGGeSS1DiDXJIaZ5BLUuMMcklqXO8gT7IkyflJvjSOgiRJszOOPfLXA+vGsB1J0gh6BXmSXYBDgI+PpxxJ0mz13SP/CPBm4N6ZFkiyOsnaJGs3bNjQczhJ0lQjB3mSQ4EbqurcjS1XVWuqalVVrZqYmBh1OEnSDPrskT8DeEGSq4FTgf2TfGYsVUmShjZykFfVW6pql6paCRwOfKuqjhxbZZKkoXgduSQ1buk4NlJVZwFnjWNbkqTZcY9ckhpnkEtS4wxySWqcQS5JjTPIJalxBrkkNc4gl6TGGeSS1DiDXJIaZ5BLUuMMcklqnEEuSY0zyCWpcQa5JDXOIJekxo3l75HPh5XHfXmhS5CkRck9cklqnEEuSY0zyCWpcQa5JDXOIJekxhnkktQ4g1ySGmeQS1LjDHJJapxBLkmNM8glqXEGuSQ1ziCXpMYZ5JLUOINckhpnkEtS40YO8iS7Jvl2knVJLkny+nEWJkkaTp9vCLobeENVnZdkW+DcJGdU1aVjqk2SNISR98ir6rqqOq+bvhVYB+w8rsIkScMZyzHyJCuBvYFzppm3OsnaJGs3bNgwjuEkSZP0DvIk2wBfAI6pqlumzq+qNVW1qqpWTUxM9B1OkjRFryBPsjmDEP9sVX1xPCVJkmajz1UrAU4C1lXVh8ZXkiRpNvrskT8DeAWwf5ILutvBY6pLkjSkkS8/rKrvARljLZKkEfjJTklqnEEuSY0zyCWpcQa5JDXOIJekxhnkktQ4g1ySGmeQS1LjDHJJapxBLkmNM8glqXEGuSQ1ziCXpMYZ5JLUOINckhpnkEtS4wxySWqcQS5JjTPIJalxBrkkNc4gl6TGGeSS1DiDXJIaZ5BLUuMMcklqnEEuSY0zyCWpcQa5JDXOIJekxhnkktQ4g1ySGmeQS1LjDHJJalyvIE9yYJLLklyR5LhxFSVJGt7IQZ5kCfCPwEHAnsARSfYcV2GSpOH02SPfF7iiqq6qqruAU4HDxlOWJGlYS3usuzPw80n31wP7TV0oyWpgdXf3tiSXddPLgRt7jL9Y2Vc7Hoo9gX0tOnnfRmcP09duG5vZJ8gzzWP1oAeq1gBrHrRysraqVvUYf1Gyr3Y8FHsC+2rNOPrqc2hlPbDrpPu7ANf2KUaSNHt9gvx/gCck2T3JFsDhwOnjKUuSNKyRD61U1d1JXgt8HVgCnFxVl8xiEw863PIQYV/teCj2BPbVmt59pepBh7UlSQ3xk52S1DiDXJIaNydBvqmP7if5cJILutuPk9w8ad77k1ySZF2Sf0gy3WWO826InlYk+XaS85NclOTgSfPe0q13WZLnzW/lGzdqX0mek+TcJBd3P/ef/+pn1uf5mjT/tiRvnL+qN63n6/DJSc7ufr8uTrLl/FY/sx6vw82TfKrrZ12St8x/9dMboqfdkpzZ9XNWkl0mzTsqyeXd7ahNDlZVY70xOPF5JfBYYAvgQmDPjSz/OgYnSgGeDny/28YS4Gzg2eOucS56YnDC4q+66T2BqydNXwgsA3bvtrNkoXsaQ197Azt1008CrlnofsbR16T5XwA+D7xxofsZ0/O1FLgIeEp3/xEPkdfhy4BTu+mtgKuBlY309HngqG56f+DT3fSOwFXdzx266R02Nt5c7JHP9qP7RwCndNMFbMmg8WXA5sAv5qDG2RqmpwK266Yfzv3X1B/G4IV2Z1X9BLii295iMHJfVXV+Vd3X4yXAlkmWzUPNw+jzfJHkhQx+eWZzFdZ86NPXc4GLqupCgKr636q6Zx5qHkafvgrYOslS4GHAXcAtc1/yJg3T057Amd30tyfNfx5wRlXdVFW/BM4ADtzYYHMR5NN9dH/n6RZMshuDvdRvAVTV2Qwauq67fb2q1s1BjbM1TE8nAEcmWQ98hcE7jWHXXSh9+prsxcD5VXXnXBQ5gpH7SrI1cCzwzrkvc9b6PF+/A1SSryc5L8mb57rYWejT12nArxnkxc+AD1TVTXNa7XCG6elCBr87AC8Ctk3yiCHXfYC5CPKhPrrfORw47b49gySPB57I4FOiOwP7J3nmHNQ4W8P0dATwyaraBTgY+HSSzYZcd6H06WuwgWQv4H3Aa+asytnr09c7gQ9X1W1zXOMo+vS1FPhD4OXdzxclOWAui52FPn3tC9wD7MRgp/ANSR47l8UOaZie3gg8K8n5wLOAa4C7h1z3Afr8rZWZzOaj+4cDfz3p/ouAH9z3S5Tkq8DTgO/OQZ2zMUxPr6Z7+1NVZ3cnkpYPue5C6dPXDd3JmX8HXllVV85DvcPq09d+wEuSvB/YHrg3yR1V9dG5L3uT+r4Ov1NVNwIk+QrwVO5/a7+Q+vT1MuBrVfUbBq/J7wOrGBwaW0ib7Kk7NPmnAEm2AV5cVb/q3nU8e8q6Z210tDk4yL+UwT/i7tx/kH+vaZbbg8GJiUx67KXAN7ttbM7gRfb8RXDiYpM9AV8FXtVNP7F70gLsxQNPdl7F4jnJ1Kev7bvlX7zQfYyzrynLnMDiOtnZ5/naATiPwQnBpd3v2SEL3dMY+joW+EQ3vTVwKfDkRnpaDmzWTb8HeFc3vSPwk+4526Gb3nGj481REwcDP2Zw1vb47rF3AS+YtMwJwIlT1lsCfAxY1z0hH1roJ2TYnhicuPh+94RdADx30rrHd+tdBhy00L2Moy/gbQyOTV4w6fbIhe5nHM/XlNfoognyMbwOj2RwAveHwPsXupcxvQ63YXD1xyVdZrxpoXuZRU8vAS7vlvk4sGzSun/G4MKIK4CjNzWWH9GXpMb5yU5JapxBLkmNM8glqXEGuSQ1ziCXpMYZ5JLUOINckhr3f8/gJTPiJaswAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "source1 = expert1_pd.copy()\n",
    "source2 = predict_pd.copy()\n",
    "\n",
    "source1_seg = [i + '.npy' for i in np.array(source1.refined_seg_path)]\n",
    "# source1_seg = list(source1.seg_path)\n",
    "\n",
    "source2_seg = [i + '.npy' for i in np.array(source2.refined_seg_path)]\n",
    "# source2_seg = list(source2.seg_path)\n",
    "\n",
    "\n",
    "# Get Dice Score \n",
    "dice_scores = get_dice_scores(source1_seg, source2_seg)\n",
    "\n",
    "# Get Jaccard Index\n",
    "jaccard_indices = get_jaccard_indices(source1_seg, source2_seg)\n",
    "\n",
    "# See how well they agree in terms of the average T2 value in each cartilage region of each image\n",
    "source1_regions = list(source1.t2_region_json_path)\n",
    "source2_regions = list(source2.t2_region_json_path)\n",
    "correlation_dict, mean_abs_diff_dict = compare_region_means(source1_regions, source2_regions, results_path=None)\n",
    "\n",
    "\n",
    "# See how well they agree in terms of the average T2 change over time in each cartilage region of each image\n",
    "source1_regions1 = [i for i in source1_regions if i[-6]=='4']\n",
    "source1_regions2 = [i for i in source1_regions if i[-6]=='8']\n",
    "source2_regions1 = [i for i in source2_regions if i[-6]=='4']\n",
    "source2_regions2 = [i for i in source2_regions if i[-6]=='8']\n",
    "\n",
    "if len(source1_regions1)==len(source1_regions2):\n",
    "    change_correlation_dict, change_mean_abs_diff_dict = compare_region_changes(source1_regions1,\n",
    "                                                                                source1_regions2, \n",
    "                                                                                source2_regions1,\n",
    "                                                                                source2_regions2, \n",
    "                                                                                results_path=None)\n",
    "plt.hist(dice_scores, bins = 4)\n",
    "plt.title(\"Dice Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(jaccard_indices, bins = 4)\n",
    "plt.title(\"Jaccard Indices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 0.40568825137207604,\n",
       " 'superficial': 0.5102981384967771,\n",
       " 'deep': 0.5133353160956593,\n",
       " 'SLA': 1.435832283184708,\n",
       " 'SLC': 1.2316775209888513,\n",
       " 'SLP': 0.8778900633798907,\n",
       " 'SUA': 1.981700806958975,\n",
       " 'SUC': 0.9058125601407949,\n",
       " 'SUP': 1.1572836315188764,\n",
       " 'DLA': 0.9914941838863768,\n",
       " 'DLC': 1.3569768929004649,\n",
       " 'DLP': 1.1993144862661858,\n",
       " 'DUA': 1.6699665143668374,\n",
       " 'DUC': 1.9641337048921395,\n",
       " 'DUP': 1.3050470322961487,\n",
       " 'LA': 1.027298868244049,\n",
       " 'LC': 0.7407185102125499,\n",
       " 'LP': 0.7816603039841389,\n",
       " 'UA': 1.521837531864047,\n",
       " 'UC': 1.0878131592927356,\n",
       " 'UP': 0.9538690813465492}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_abs_diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_functions import dice_loss_test_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss_test_volume(np.flip(elka, axis = 0), qmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('/data/kevin_data/expert2/segmentations'):\n",
    "    if file[-4:]=='.npy':\n",
    "        temp = np.load(os.path.join('/data/kevin_data/expert2/segmentations',file))\n",
    "        temp = np.flip(temp, axis = 0)\n",
    "        np.save(os.path.join('/data/kevin_data/expert2/segmentations',file),temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
